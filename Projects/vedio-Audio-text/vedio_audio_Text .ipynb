{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa10f4f-76d7-44c7-85bf-311ba42b21e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from moviepy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from moviepy) (2.32.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from moviepy) (2.33.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (69.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede26218-2097-40cf-940a-8ff984129faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in F:\\AI-DSA012 Day-35 29th July audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Function to extract audio from video\n",
    "def extract_audio_from_video(video_path, audio_output_path):\n",
    "    # Load the video file\n",
    "    video = VideoFileClip(video_path)\n",
    "    \n",
    "    # Extract the audio\n",
    "    audio = video.audio\n",
    "    \n",
    "    # Save the audio to a file\n",
    "    audio.write_audiofile(audio_output_path)\n",
    "\n",
    "# Example usage\n",
    "video_path = 'D:\\chaitanya\\D drive\\OKEN SCANER\\DSA012\\AI-DSA012 Day-35 29th July.mp4'  # Path to your video file\n",
    "audio_output_path = 'F:\\AI-DSA012 Day-35 29th July audio.mp3'  # Path where the extracted audio will be saved\n",
    "\n",
    "extract_audio_from_video(video_path, audio_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6b06c6-62e7-4a89-b527-c5ef62f99f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not request results from Google Speech Recognition service; recognition request failed: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Set the path for ffmpeg\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\ffmpeg\\ffmpeg-2024-10-24-git-153a6dc8fa-full_build\\bin'\n",
    "\n",
    "# Path to your MP3 file\n",
    "audio_path = r\"F:\\Testing for text\\AI-DSA012 Day-35 29th July audio.mp3\"\n",
    "\n",
    "# Convert MP3 to WAV using pydub\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "wav_path = audio_path.replace('.mp3', '.wav')\n",
    "audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Load the converted WAV file\n",
    "with sr.AudioFile(wav_path) as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "\n",
    "    # Recognize the speech from the audio\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(\"Extracted Text:\\n\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f0d341-1d0a-48c9-a8e4-3fa8a430afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition could not understand chunk 0\n",
      "Text from chunk 1:\n",
      "good morning how was your weekend\n",
      "\n",
      "Text from chunk 2:\n",
      "to all of you open your notes today like I want most of you to write notes ok so today we will start on machine learning ok just for today I just tell talk about couple of basic categories of machine learning and what we do in machine learning expect you to have lots of doubts in the second let's start you are training OK what else see my laptop\n",
      "\n",
      "Text from chunk 3:\n",
      "ok so whenever you create a chatbot it's a program right ultimately it's a program it has some functions of water it might be ultimately it's a program right so what is thus come to your mind OK you don't know what you are going to train model looks like\n",
      "\n",
      "Text from chunk 4:\n",
      "you have some history of data and you need to predict some future some future or some risk associated with the future ultimately that's the agenda all of you would agree so can I define training ok learning patterns from past and predicting future and that means my definition or information from past and future what do you think algorithm function to sort or to perform\n",
      "\n",
      "Text from chunk 5:\n",
      "similarly machine learning algorithms are also some predefined functions for predefined set of algorithms which takes some data to credit some information output ok so when I want to predict the data type that I have can be two types right numerical data types what are all the nominal for example today if I ask about continuous series of numbers ok so you can define your machine learning models into two types\n",
      "\n",
      "Text from chunk 6:\n",
      "OK regressions will deal with predicting continuous data classification deal with true or false is only binary right if there are three categories what would you do then it's not true or false it's 183 categories so there can be n categories ok so machine learning algorithm typically Falls into two categories regulation is about prediction of continuous variables and classification is about protection of categories categorical variables now if I take my age if I take my height and if I ask you to compute the predict the weight take the height of a person\n",
      "\n",
      "Text from chunk 7:\n",
      "can I do that it can be simple equation right age versus height can I call that machine learning algorithm yes or no problem simple production\n",
      "\n",
      "Text from chunk 8:\n",
      "so you are machine learning can algorithm can simply be a simple rule has a line drawing lots and lots of equations with it ultimately at the end of the day of machine learning model is a set of application and variable set and all you do is existing the perimeter model regression and classification of classification\n",
      "\n",
      "Text from chunk 9:\n",
      "a target variable to predict a target variable to predict now if what happens if you don't have a target variable to protect now if I give you this information about all of you people students education details and few more and I if I want to group people into groups I want to categories how can you group yourself education\n",
      "\n",
      "Text from chunk 10:\n",
      "BTech I can group you want to one group group 2 can be of degree can be of any other education detail Ok or else I can pick age or else I can pick any other I don't have a target all I need to understand this how to group you people ok this type of understanding what this type of modelling is called unsupervised learning where I don't have a target to predict that it is called model\n",
      "\n",
      "Text from chunk 11:\n",
      "problem where your data set will not have any target variable what is the phone yes based on toes you can cluster toys into group side if they fall into particular dimension what will you do with image\n",
      "\n",
      "Text from chunk 12:\n",
      "that is again the image has a dog you need to label it has a dog if the image has that you are labelling it as cat it's not like grouping or doing anything so it is again supervised machine learning algorithm you have you need to have a label for that any other data set which can be categorised as unsupervised data set I want you to come up with data centres and song\n",
      "\n",
      "Text from chunk 13:\n",
      "ok so if the target is unknown and you want to group the data into clusters then it becomes unsupervised whereas in supervised cases you will have a clear that target to predict for example who wins tomorrow's matches ok or else what is what is the angle that is present in this particular image that of dog that becomes your voice is how can you\n",
      "\n",
      "Text from chunk 14:\n",
      "large language models so they are there you need to think about the training data set of that particular model ok so this models are trained on huge text data right choose test data and the prediction that happens within the chat box predicting next word predicting next word so if I give you text for example previously can you make that can you build a model\n",
      "\n",
      "Text from chunk 15:\n",
      "what all information do we have in that no no I am asking about where is the insurance file now in this data set\n",
      "\n",
      "Text from chunk 16:\n",
      "if I want to print the charges using this information would be become which category of regression classification one person one person characteristics\n",
      "\n",
      "Text from chunk 17:\n",
      "that type of algorithm where you don't have a target groups and optimise those groups in such that each group represents a unique group for example split this entire collection for two groups the best way to splitter this girls and boys all girls you know similar in terms of gender in terms of reduce\n",
      "\n",
      "Text from chunk 18:\n",
      "groups of people like different categories of people based on characteristics for example will have certain age people will have certain BMI range can I group people based upon this four different groups moderate using problem\n",
      "\n",
      "Text from chunk 19:\n",
      "regression and classification models technicians and classification models what is supervised and unsupervised the other one is regression and classification ok do you want any further explanation with this ok now different stages of machine learning model for life cycle of gathering business requirements what do you mean by election\n",
      "\n",
      "Text from chunk 20:\n",
      "post office to understand what my problem is what is my business stakeholders asking from ok I am a solving the happiness problem or I am a solving Pharmaceutical problem you know you need to exactly understand what you requirement is what do you want to produce ok if I want to produce stock price what all data do you need let's say if I want to predict stock price tomorrow\n",
      "\n",
      "Text from chunk 21:\n",
      "I ask you to predict whether if you are experiencing would say you require satellite image OK so the type of data can vary but the first step is to understand what do you want to predict ok do you want to predict whether or do you want to print premium Insurance claim this year will my insurance\n",
      "\n",
      "Text from chunk 22:\n",
      "will I have to pay more or less today this year I pay 10000 I got into an accident ok I claim 30000 my entire repair posted 300003 that Insurance Company paid at 7:30 a.m.\n",
      "\n",
      "Text from chunk 23:\n",
      "which never got into single accident ok don't 6 years it has never been into an accident I never claimed insurance ok now my friend as a car which is 6 years old and he got into accept every year every year he claims insurance\n",
      "\n",
      "Text from chunk 24:\n",
      "my friend should be charged ok I ask you to build a machine learning model around that you will ask a particular set of information right in order to build such model which credit vehicle insurance premium ok you will build a modern you need some data to train that model of your target might be premium amount but the data to train that model you need to understand OK what will you connect\n",
      "\n",
      "Text from chunk 25:\n",
      "ok at least in US everything is monitor even if one Paracetamol the data is recorded somewhere in India we don't care you want from Medical shop of medicines not beyond that ok so in India if I want to build a model today today problems\n",
      "\n",
      "Text from chunk 26:\n",
      "price of vehicle yes exactly so if it is very costly and accident the repair cost would be very high so price of vehicle what is ok kilometre traffic what is model in the science really great\n",
      "\n",
      "Text from chunk 27:\n",
      "why should I worry about pollution check\n",
      "\n",
      "Text from chunk 28:\n",
      "OK what else Ok now you have seven details with you let's stop here now you have 7 details with you not all of them will be important to complete the premium write some of them will be imported and some of them will not be important we don't know which are important yet but based upon your requirement\n",
      "\n",
      "Text from chunk 29:\n",
      "which is not important pyramid all you do is which of parameters you think is important you will use that ok now exploratory data analysis the third step is exploratory data analysis but once you gather data once you get all the information now you need to perform draw some analysis that we have talked about in statistical statistics and probability like talked about should I take 10000 peoples information should I take 20000 peoples information to build a model is that you will be discovered relation between your production and your input variables now your target\n",
      "\n",
      "Text from chunk 30:\n",
      "insurance premium ok now you will try to understand ok is any particular variable doesn't have any information are there null values does the data have any quality vidmate is there any relation between vehicle model and fuel type is there any relation between those or you know previous history of accidents and maybe a couple of other variables do they have any relation with them how much percent of vehicles are automatic and how much percentage of vehicles are you know how many hours 2 years old how many years old so you need to have all the statistics exploring your data how does\n",
      "\n",
      "Text from chunk 31:\n",
      "ok I got this data through process is my data reliable customer data have any quality to it ok all everything comes into exploratory data analysis data cleaning filling up your missing values ok identify once you are done with accelerated analysis which involves in cleaning your data building your features of modifying your data to certain and if it\n",
      "\n",
      "Text from chunk 32:\n",
      "supervised model ok now if if you know that the data is labelled then you would say you would ask a question ok is my data continue is my target continuous or categorical if my data like if my target is continuous I would pick a regression model with my data is category target is categorical then I would pick a classification models and classification model\n",
      "\n",
      "Text from chunk 33:\n",
      "let's say I am thinking of example ok let's I will show how to model training and evaluation and presentation works through an example ok understand now\n",
      "\n",
      "Text from chunk 34:\n",
      "in Dollars ok the price also should ideally go upright small house will be more water is equal to\n",
      "\n",
      "Text from chunk 35:\n",
      "MX Players you will have flat cost ok now what am I optimising so you would be measure the distance\n",
      "\n",
      "Text from chunk 36:\n",
      "distance of summation post download so that moment is called Optimisation and revaluation ok Google ok so that's what will happen\n",
      "\n",
      "Text from chunk 37:\n",
      "237709 ok now your line over there represent your machine learning English from your line to each point you are able to credit it better ok now\n",
      "\n",
      "Text from chunk 38:\n",
      "input of size of house would be price of house represents which can produce so here you see if you move your price the moment you change your X value your Y value is 25156 and C value is 271\n",
      "\n",
      "Text from chunk 39:\n",
      "based upon my X value my price of house Paris business model it's the model name is called linear regression all you are trying to wish draw a line ok design a line which represents your which can predict your outcome ok and weight for height and weight of people I can draw can you tell me which is the password if it means what has happened\n",
      "\n",
      "Text from chunk 40:\n",
      "ok now will the price for Bigg Boss like you know if the price goes up will my life\n",
      "\n",
      "Text from chunk 41:\n",
      "fire accidents and number of five digits deployed ok so that rod number of fire accidents on y-axis and number of fire engines deployed on x-axis ok and they found out if they are deploying more fire engines more fire accidents are happening that was the outcome of the assumption asking you to reduce the number of five digits between deployed would you agree so you need to understand what is the correct what is the effect if I tell you your height is dependent on your job would you agree can you predict your height using your job stupid reason\n",
      "\n",
      "Text from chunk 42:\n",
      "for some stupid reason you are able to predict your height using your job would you accept that why accept ok now if I pick software engineer ok managers employers and all these peoples is the height have any correlation with their salary are there now for some reason for some reason if you use salary and if you are able to protect height of the house\n",
      "\n",
      "Text from chunk 43:\n",
      "for the variation in my price of house would you agree I can use that that makes business sense tomorrow for a business person he can understand ok if I reduce the size of the house ok I can price my house lesson based upon that we might be able to take some business action you can be say in this in his next construction project to increase or decrease the size of the property always keep in mind your input variables should be cause for the outcome if it doesn't make any sense do not include them stupid reason you are able to predict the output using that input then you should be able to clarified why you are able to predict the output using that input\n",
      "\n",
      "Text from chunk 44:\n",
      "ok for example years of car like you know how many years old the car is my premium dependent on that it is dependent how is it dependent why any justify that reason if you are if you have the justification that sounds ok ok if you do not have that business justification do not try to use OK always have clarity from your business now this is about your model evaluation is a combination of equations you don't know Black Box it can be simple prices 80 rupees you can blindly say ok tomorrow is equal to X\n",
      "\n",
      "Text from chunk 45:\n",
      "skills and salary ok can you try to correlation yes right if you have really strong skills then your salary should be higher ok so your target input variable which is text now you are parameters in your model that is about your model training and for every model we use distance from your line to that point if I want to predict the prices of tomatoes KG tomatoes my actual price minus\n",
      "\n",
      "Text from chunk 46:\n",
      "today's tomato spices are like Rs 70 and I predicted Rs 80 the distance is 10 rupees right I need to optimise that I need to reduce that by altering some parameters in my model so that my friend ok is actual prices - 3 8 5 0\n",
      "\n",
      "Text from chunk 47:\n",
      "historical data you pick up historical data and you would say price of tomatoes are dependent on these these values ok so for tomorrow if you want to create the prices of tomatoes and you are like you need to understand what your business owner is trying to do ok trading itself will not be your agenda based upon the tradition you need to take some action ok if you are cold storage manager ok tomorrow prices goes up you want to store more and more tomato if the price of you don't want to show more and more tomatoes you want to distribute you want to show something else Ok so based upon the price to predict you can recommend ok recommend any action or not to reduce all of your stop\n",
      "\n",
      "Text from chunk 48:\n",
      "if you are if you operation says that tomorrow price is going to fall what you would use payment if you sell the tomatoes you would lose money so do not sell keep them for few more days till the price goes up ok so based upon your model you will predict something and based upon the protections you will recommend some actions to your business stakeholder so will come in two stages primary one is your gathering requirements\n",
      "\n",
      "Text from chunk 49:\n",
      "I have collected this data and I have found this insights can I use this data to build a model or can I use this data to build a model or not so he will recommend ok don't use number of years ok or else also includes this particular information like you know how many times did the person get it get caught in drunk and drive test business model presentation\n",
      "\n",
      "Text from chunk 50:\n",
      "stages will take almost 80% of your if you have 10 days to build a model of this ok the largest\n",
      "\n",
      "Text from chunk 51:\n",
      "create a presentation life cycle model selection model training is the least time consuming part of the time goes to data collection and exploratory data analysis OK Google whatever I have collected does not make any difference\n",
      "\n",
      "Text from chunk 52:\n",
      "using required parameters taking this bucket one features right for data values ok so you will have some time selecting what are the important parameters that you need to cancel 90% of the times no matter what you do people are really related\n",
      "\n",
      "Text from chunk 53:\n",
      "can only give you encoded data or else I will give you need to come to my previous and perform your model on the data that I give or they will ask you I'll give you dummy data build a model I will take the model and my original data so with dummy data you have to design a model designer client he will run it on this data and only importance\n",
      "\n",
      "Text from chunk 54:\n",
      "ok if you register on the app we will directly get that information ok even if the product belong access the data because of privacy t-shirts OK your name your education details for your pan card number of your savings for all of this data is really valuable in gaming industry\n",
      "\n",
      "Text from chunk 55:\n",
      "because of this game I haven't bankrupt and committed suicide in India case we have heard a lot of issues right after those issues pubg has introduced this schema saying that ok then there will be a pop up saying that you have played one and half please stop please please take some time so that data collection all the information\n",
      "\n",
      "Text from chunk 56:\n",
      "100000 so determining the ceiling like you know your CIBIL score will help me to determine that CD but no one would give me your CIBIL score information is your pay slip or your monthly income if I know how much you earn per month I can create a ceiling ok you do not have much ok if you earn 1 lakh per month you can easily spend 10000 can you increase you know\n",
      "\n",
      "Text from chunk 57:\n",
      "ok but can I access it know if I have access to your pan card details I can put a pan card in this paisabazaar there are experience like you know if there is this company called experience if I give you your pan card details they will give you ok but there is this OTP issue you should always parameters are really important\n",
      "\n",
      "Text from chunk 58:\n",
      "to predict and to take an action getting the data is really ok how many of you use your option list while registering to any app h parameter for example today to register this to this phone call a typical use 9 install my original phone number the same way how many of you use your original age while resisting to any app status my one of my primary inputs to predict some outcome to improve your quality\n",
      "\n",
      "Text from chunk 59:\n",
      "ok so that all of that comes into your data collected data analysis in information within it or not is it raining sense or not ok so when I work for this shipping industry shipping industry you\n",
      "\n",
      "Text from chunk 60:\n",
      "for example crude oil carrying vessel ok ship carries crude oil from Dubai to India ok and a shipping boat in China it also has same number has this one I can use only that number to identify ok this number belongs to this particular presentation\n",
      "\n",
      "Text from chunk 61:\n",
      "how to collect the data into the quality and storage effect and today if you go to auto ml or even Amazon or Microsoft just by giving the data itself if you upload your data they will automatically suggest ok this particular model is relevant to you and if you train this model these are the Evaluation Matrix and these are the important features the sensitivity of the output will depend on this features from Data scientist\n",
      "\n",
      "Text from chunk 62:\n",
      "unit 7 details right to credit insurance premium now you will you tell seven parameters I need them so you need to create a table with these parameters so he created table in database you want to create and you will be accessing that information from your database accessing your information from your database so always keep in mind that you really need to have a strong understand the quality of the data yourself\n",
      "\n",
      "Text from chunk 63:\n",
      "how can oh that's about different life cycle of a machine learning types of machine learning you will have a target variable types\n",
      "\n",
      "Text from chunk 64:\n",
      "number of models and unsupervised machine learning ok and all of them are supervised ok for example if I take a sentence of 10 words I can use the first five words to print 6 the word so every 6 hours will become your target variable and you are five words so an example of algorithm would be your University application\n",
      "\n",
      "Text from chunk 65:\n",
      "a group applications together I would use their education there is the experience and all and I would do people ok people makes more sense and heterogeneous homogeneous and I would take blood group ok so in gaming one of the examples of financial services learning is trying to categorise your players as risky and non disclosure I can tell which you are bad right to categories\n",
      "\n",
      "Text from chunk 66:\n",
      "I am trying to categorise which is unable to information Matrix let you know how did he play in the first round how did you play in the second what is his total count and all different methods I can use different Matrix to group at the end of the day but the group today's class I just wanted to cover yes\n",
      "\n",
      "Text from chunk 67:\n",
      "ok hotel room price in the price of hotel rooms will be your target variable ok now\n",
      "\n",
      "Text from chunk 68:\n",
      "previous years bottle rate for you cannot use them right so ideally what you would use is how much the hotel prices went up in the countries which hosted Olympics ok so you would collect data from those portals so for example last year like you know you can expect the number of people to come to Paris you will know how many photos are there you know how many hotels are there\n",
      "\n",
      "Text from chunk 69:\n",
      "how many people would need portals based upon this energy they would price the hotel prices ok so that the problem becomes how many people would visit Olympics this year in Paris can you predict that if possible ok so in that how many of our which class people how many will be middle class people OK so how many class portal and how many people so if you are able to print that demand how many people visited Olympics\n",
      "\n",
      "Text from chunk 70:\n",
      "how many people visited cut can you come to a Census can you predict how many people would visit Paris Olympics based upon this history of events how many people did visit football how many people did visit cricket world cup and so on and you will have multiple Matrix to credit problem for example yesterday news came out the number of people who is going to visit Paris Olympics\n",
      "\n",
      "Text from chunk 71:\n",
      "again as I told you this model is very complex like for example I can book a hotel room one month ahead ok now let's assume that my hotel has 100 rupees 100 rooms ok now I predicted like you know there is too much for Olympics history information the rate of hotel booking\n",
      "\n",
      "Text from chunk 72:\n",
      "change my price ok so your input variable for machine learning model would be the rate at which people book your hotel price hotel rooms ok again if you want to leave them into that ok I'll reserve some hotel rooms for the last minute flight price\n",
      "\n",
      "Text from chunk 73:\n",
      "petrol prices you can't database say that ok these are the parameters for me to credit Hotel prizes you can't do that there are lots of this change the number of Hotel rooms and you reserve for yourself will also depend upon your business problem your tourist density in that particular area as a total number of students that is wrong\n",
      "\n",
      "Text from chunk 74:\n",
      "Olympic in parasite is it turn the third Olympic in Paris I think it is I think it is at a second ok so you not to print the room prices what would I do I would compare it with riyo Olympic hotel prices yes how much you need to help\n",
      "\n",
      "Text from chunk 75:\n",
      "800k people were stuck in stations because of lack of trains that would impact both my bottle prices and ticket prices are train ticket prices ok how should I increase my hotel prices or decrease my hotel prices hotels which are around railway stations increase the prices which are near\n",
      "\n",
      "Text from chunk 76:\n",
      "I'll design a bot to play chess ok the outcome the bottle play random game previously when it is untrained all it was this how to move that form it will move it will make some more and it lose the game OK I will add a penalty whenever it closest based upon the outcome and adding some penalty or some changes to the training data itself because your data doesn't have the labelled information so what will try all the permutations if it doesn't\n",
      "\n",
      "Text from chunk 77:\n",
      "it's ok if it gets some few things in any other terminals\n",
      "\n",
      "Text from chunk 78:\n",
      "categories for example let's say you have 100 documents and you want to categorise them into document which talk about cars and documents which talk about ok now initially when you start the data itself so what would you do you label some percent of 20 document information\n",
      "\n",
      "Text from chunk 79:\n",
      "no person will ask you for semi supervised let me know I would say no in the first time if you have labels it is the boys if you don't have labels it is also provide mandatory\n",
      "\n",
      "Text from chunk 80:\n",
      "That's How You transform yourself\n",
      "\n",
      "Text from chunk 81:\n",
      "you will cover that in the planning education to do something ok all of you heard of new budget right all of you have heard of new budget so based upon the Purchase I need you I need you to create an excel file for what salary how much you need to collect information about the tax slabs various components of your salary how much should be how much proportion will go to each for example\n",
      "\n",
      "Text from chunk 82:\n",
      "Excel should take the amount of time for class thank you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "# Set the path for ffmpeg\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\ffmpeg\\ffmpeg-2024-10-24-git-153a6dc8fa-full_build\\bin'\n",
    "\n",
    "# Path to your MP3 file\n",
    "audio_path = r\"F:\\Testing for text\\AI-DSA012 Day-35 29th July audio.mp3\"\n",
    "\n",
    "# Convert MP3 to WAV using pydub\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "wav_path = audio_path.replace('.mp3', '.wav')\n",
    "audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "# Split audio into smaller chunks (e.g., 1 minute each)\n",
    "chunk_length_ms = 60 * 1000  # 60 seconds (1 minute)\n",
    "chunks = math.ceil(len(audio) / chunk_length_ms)  # Calculate the number of chunks\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to transcribe each chunk of audio\n",
    "def transcribe_chunk(audio_chunk, chunk_index):\n",
    "    # Export the current chunk to a WAV file\n",
    "    chunk_path = f\"chunk{chunk_index}.wav\"\n",
    "    audio_chunk.export(chunk_path, format=\"wav\")\n",
    "    \n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(f\"Text from chunk {chunk_index}:\\n{text}\\n\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(f\"Google Speech Recognition could not understand chunk {chunk_index}\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service for chunk {chunk_index}; {e}\")\n",
    "\n",
    "# Loop over each chunk and transcribe it\n",
    "for i in range(chunks):\n",
    "    start_time = i * chunk_length_ms\n",
    "    end_time = min((i + 1) * chunk_length_ms, len(audio))\n",
    "    audio_chunk = audio[start_time:end_time]  # Extract chunk\n",
    "    transcribe_chunk(audio_chunk, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0535d483-0332-4abb-b021-f2958c5e173b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='F:\\\\Testing for text\\\\AI-DSA012 Day-35 29th July audio.wav'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert MP3 to WAV (Mono, 16kHz)\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "audio = audio.set_channels(1)  # Convert to mono\n",
    "audio = audio.set_frame_rate(16000)  # Set the frame rate to 16kHz\n",
    "wav_path = audio_path.replace('.mp3', '.wav')\n",
    "audio.export(wav_path, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ce4fc3-ab30-4b2e-9481-fd4475bdb471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not request results from Google Speech Recognition service; recognition request failed: Bad Request\n"
     ]
    }
   ],
   "source": [
    "with sr.AudioFile(wav_path) as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        # Set language to 'en-US' for US English, or adjust as needed for other languages\n",
    "        text = recognizer.recognize_google(audio_data, language='en-US')\n",
    "        print(\"Extracted Text:\\n\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a01b999-a264-4d1a-b59b-7d0713a10d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not request results from Google Speech Recognition service; recognition request failed: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Set the path for ffmpeg\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\ffmpeg\\ffmpeg-2024-10-24-git-153a6dc8fa-full_build\\bin'\n",
    "\n",
    "# Path to your MP3 file\n",
    "audio_path = r\"F:\\Testing for text\\AI-DSA012 Day-35 29th July audio.mp3\"\n",
    "\n",
    "# Convert MP3 to WAV (Mono, 16kHz)\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "audio = audio.set_channels(1)  # Convert to mono\n",
    "audio = audio.set_frame_rate(16000)  # Set the frame rate to 16kHz\n",
    "wav_path = audio_path.replace('.mp3', '.wav')\n",
    "audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Load the converted WAV file for speech recognition\n",
    "with sr.AudioFile(wav_path) as source:\n",
    "    # Record the audio from the file\n",
    "    audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        # Recognize the speech using Google's Speech Recognition API\n",
    "        # Set language to 'en-US' for US English (adjust if necessary for other languages)\n",
    "        text = recognizer.recognize_google(audio_data, language='en-US')\n",
    "        print(\"Extracted Text:\\n\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        # If the speech is not recognized\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        # If there's an issue with the Google API request\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f2d9cd9-c989-4ce9-ab51-c34bd60ac08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition could not understand chunk 0\n",
      "Text from chunk 1:\n",
      "good morning Google how was your weekend\n",
      "\n",
      "Text from chunk 2:\n",
      "to all of you open your notes today like I want most of you to write notes ok so today we will start on machine learning ok just for today it with your early hearing I just tell talk about couple of basic categories of machine learning and what we do in machine learning expect you to have lots of doubts in the second let's start your training\n",
      "\n",
      "Text from chunk 3:\n",
      "ok so whenever you create a chatbot it's a program right ultimately it's a program it has some functions of water it might be ultimately it's a program right so what is thus come to your mind OK you don't know what you are going to train model looks like\n",
      "\n",
      "Text from chunk 4:\n",
      "you have some history of data and you need to predict some future some future or some risk associated with the future ultimately that's the agenda all of you would agree so can I define training ok learning patterns from past and predicting future and that means my definition learn patterns or information from past and present future machine learning algorithm what do you think to sort or to perform\n",
      "\n",
      "Text from chunk 5:\n",
      "similarly machine learning algorithms are also some predefined functions for predefined set of algorithms which takes some data to predict some information output ok so when I want to predict the data type that I have can be two types right numerical data types what are all that nominal for example today if I ask about its nominal series of numbers ok so you can define your machine learning models into two types\n",
      "\n",
      "Text from chunk 6:\n",
      "OK regressions will deal with predicting continuous data classifications deal with true or false is only binary right if there are three categories what would you do then it's not true or false it's 83 categories so there can be n categories ok so machine learning algorithm typically Falls into two categories one is regression classification is about prediction of continuous variables and classification is about protection of categories categorical variables now if I take my age if I take my height and if I ask you to compute the predict the weight of a person\n",
      "\n",
      "Text from chunk 7:\n",
      "can I do that it can be simple equation right age versus height would you agree can I call that machine learning algorithm yes or no problem so you have one input variable and one output\n",
      "\n",
      "Text from chunk 8:\n",
      "so you are machine learning can algorithm can simply be a simple rule has a line drawing line lots and lots of equations with it ultimately at the end of the day of machine learning model is a set of equations a particular class\n",
      "\n",
      "Text from chunk 9:\n",
      "a target variable to predict a target variable to predict now if what happens if you don't have a target variable to protect now if I give you this information about all of you people students education details and few more and I if I want to group people into groups I want to categories how can you improve yourself using some category of information education\n",
      "\n",
      "Text from chunk 10:\n",
      "BTech I can group you want to one group group 2 can be of degree can be of any other education detail Ok or else I can pick age or else I can pick any other I don't have a target all I need to understand is how to group you people ok this type of understanding what type of modelling is called unsupervised learning where I don't have a target if I have a target to predict that it is called model\n",
      "\n",
      "Text from chunk 11:\n",
      "problem where your data set will not have any target variable what is the phone yes based upon sizes of toys you can cluster toys into group side if they fall into particular dimension what will you do\n",
      "\n",
      "Text from chunk 12:\n",
      "that is again if the image has a dog you need to label it has a dog if the image has that you are labelling it as cat it's not like grouping or doing anything so it is again supervised machine learning algorithm you have you need to have a label for that any other data set which can be categorised as an supervised data set I want you to come up with data centre in the data set itself this is animal number one number\n",
      "\n",
      "Text from chunk 13:\n",
      "ok so if the target is unknown and you want to group the data into clusters then it becomes unsupervised whereas in supervised cases you will have a clear that target to predict for example who wins tomorrow's matches ok or else what is what is the angle that is present in this particular image cat of dog that becomes your voice what do you think the background how can you\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m chunk_length_ms, \u001b[38;5;28mlen\u001b[39m(audio))  \u001b[38;5;66;03m# Make sure to not go beyond the audio length\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     audio_chunk \u001b[38;5;241m=\u001b[39m audio[start_time:end_time]  \u001b[38;5;66;03m# Extract chunk\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     chunk_text \u001b[38;5;241m=\u001b[39m transcribe_chunk(audio_chunk, i)  \u001b[38;5;66;03m# Transcribe the chunk\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     all_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Append the text from each chunk\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Save the complete transcription to a text file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 33\u001b[0m, in \u001b[0;36mtranscribe_chunk\u001b[1;34m(audio_chunk, chunk_index)\u001b[0m\n\u001b[0;32m     30\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecord(source)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Recognize the speech using Google's Speech Recognition API\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     text \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_google(audio_data, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men-US\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText from chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:256\u001b[0m, in \u001b[0;36mrecognize_legacy\u001b[1;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[0m\n\u001b[0;32m    251\u001b[0m request_builder \u001b[38;5;241m=\u001b[39m create_request_builder(\n\u001b[0;32m    252\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint, key\u001b[38;5;241m=\u001b[39mkey, language\u001b[38;5;241m=\u001b[39mlanguage, filter_level\u001b[38;5;241m=\u001b[39mpfilter\n\u001b[0;32m    253\u001b[0m )\n\u001b[0;32m    254\u001b[0m request \u001b[38;5;241m=\u001b[39m request_builder\u001b[38;5;241m.\u001b[39mbuild(audio_data)\n\u001b[1;32m--> 256\u001b[0m response_text \u001b[38;5;241m=\u001b[39m obtain_transcription(\n\u001b[0;32m    257\u001b[0m     request, timeout\u001b[38;5;241m=\u001b[39mrecognizer\u001b[38;5;241m.\u001b[39moperation_timeout\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    260\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m OutputParser(\n\u001b[0;32m    261\u001b[0m     show_all\u001b[38;5;241m=\u001b[39mshow_all, with_confidence\u001b[38;5;241m=\u001b[39mwith_confidence\n\u001b[0;32m    262\u001b[0m )\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_parser\u001b[38;5;241m.\u001b[39mparse(response_text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:222\u001b[0m, in \u001b[0;36mobtain_transcription\u001b[1;34m(request, timeout)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognition connection failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m.\u001b[39mreason)\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_chunked(amt)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:595\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    593\u001b[0m value \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 595\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (chunk_left \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_chunk_left()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_left:\n\u001b[0;32m    597\u001b[0m             value\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(amt))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:579\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_next_chunk_size()\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:539\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Set the path for ffmpeg\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\ffmpeg\\ffmpeg-2024-10-24-git-153a6dc8fa-full_build\\bin'\n",
    "\n",
    "# Path to your MP3 file\n",
    "audio_path = r\"F:\\AI-DSA012 Day-35 29th July audio.mp3\"\n",
    "\n",
    "# Convert MP3 to WAV (Mono, 16kHz)\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "audio = audio.set_channels(1)  # Convert to mono\n",
    "audio = audio.set_frame_rate(16000)  # Set the frame rate to 16kHz\n",
    "\n",
    "# Define chunk length in milliseconds (e.g., 1 minute per chunk = 60 seconds = 60000 milliseconds)\n",
    "chunk_length_ms = 60 * 1000  # 60 seconds (1 minute) per chunk\n",
    "total_chunks = math.ceil(len(audio) / chunk_length_ms)  # Total number of chunks\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to transcribe each chunk of audio\n",
    "def transcribe_chunk(audio_chunk, chunk_index):\n",
    "    chunk_path = f\"chunk{chunk_index}.wav\"\n",
    "    audio_chunk.export(chunk_path, format=\"wav\")  # Export each chunk as WAV\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            # Recognize the speech using Google's Speech Recognition API\n",
    "            text = recognizer.recognize_google(audio_data, language='en-US')\n",
    "            print(f\"Text from chunk {chunk_index}:\\n{text}\\n\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(f\"Google Speech Recognition could not understand chunk {chunk_index}\")\n",
    "            return \"\"\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service for chunk {chunk_index}; {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# Split audio into smaller chunks and transcribe each chunk\n",
    "all_text = \"\"\n",
    "for i in range(total_chunks):\n",
    "    start_time = i * chunk_length_ms\n",
    "    end_time = min((i + 1) * chunk_length_ms, len(audio))  # Make sure to not go beyond the audio length\n",
    "    audio_chunk = audio[start_time:end_time]  # Extract chunk\n",
    "    chunk_text = transcribe_chunk(audio_chunk, i)  # Transcribe the chunk\n",
    "    all_text += chunk_text + \" \"  # Append the text from each chunk\n",
    "\n",
    "# Save the complete transcription to a text file\n",
    "output_text_path = \"transcription.txt\"\n",
    "with open(output_text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(all_text)\n",
    "\n",
    "print(f\"Complete transcription saved to {output_text_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb1273a-c7de-4c85-916d-077a8cdabe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: audio_chunk_1.wav\n",
      "Exported: audio_chunk_2.wav\n",
      "Exported: audio_chunk_3.wav\n",
      "Exported: audio_chunk_4.wav\n",
      "Exported: audio_chunk_5.wav\n",
      "Exported: audio_chunk_6.wav\n",
      "Exported: audio_chunk_7.wav\n",
      "Exported: audio_chunk_8.wav\n",
      "Exported: audio_chunk_9.wav\n",
      "Exported: audio_chunk_10.wav\n",
      "Audio has been split into 10 chunks successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Set the path for ffmpeg if needed\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\ffmpeg\\ffmpeg-2024-10-24-git-153a6dc8fa-full_build\\bin'\n",
    "\n",
    "# Path to your audio file\n",
    "audio_path = r\"F:\\Testing for text\\AI-DSA012 Day-35 29th July audio.mp3\"\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "\n",
    "# Determine the duration of the audio file in milliseconds\n",
    "duration_ms = len(audio)\n",
    "\n",
    "# Calculate the length of each split (in milliseconds)\n",
    "split_length = duration_ms // 10\n",
    "\n",
    "# Create and export each split\n",
    "for i in range(10):\n",
    "    start_time = i * split_length\n",
    "    end_time = start_time + split_length if i < 9 else duration_ms  # Ensure the last chunk goes to the end\n",
    "    audio_chunk = audio[start_time:end_time]\n",
    "    \n",
    "    # Export each chunk as a separate WAV file\n",
    "    chunk_filename = f\"audio_chunk_{i + 1}.wav\"\n",
    "    audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "    print(f\"Exported: {chunk_filename}\")\n",
    "\n",
    "print(\"Audio has been split into 10 chunks successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c848b64-0f73-44a8-8145-a55b86bacb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from chunk 0:\n",
      "a matrix decomposition methods we will go through them how like go through how Matrix decomposition works but the first technique is called latent semantic analysis ok LSA or latent semantic analysis so the technique it applies is single value decomposition I'll walk you through what single value decomposition exactly means OK so the idea is it identifies the topic within your text documents by decomposing the term Matrix document from document Matrix into three matrices does anyone remember term document Matrix that we have created in the last class on Saturday we have created a vector Matrix write the presence of the word frequency are also looked at the frequency and Inverse document\n",
      "\n",
      "Text from chunk 1:\n",
      "three styles we have to get right so you can pick any any particular Matrix let you know it can be a matrix it can be termed sorry term Matrix or term frequency matrix of TF idf Matrix which are Matrix you want to pay you can call that term document Matrix ok so can anyone tell me what would be the dimensions of your term Matrix can anyone tell me what would be the dimensions of your term Matrix not remember exactly so the dimensions would be if you know number of documents on one side that will be your number of rows and number of terms of copper size would be on your column size you create a vector of your car\n",
      "\n",
      "Text from chunk 2:\n",
      "for each document will be creating a vector so if you come in all the vectors you will have how many factors the number of document vectors ok so this is how it looks like so this is your words like you know your entire Corpus this is your document so it will be looking at the word presence of PF ID of metric along the entire Matrix this will be called your document Matrix and the idea is to break that Matrix into three different matrix matrix ok so first Matrix would be your like you know again the dimensions are completely dependent on your number of topics that you want to break your at the documents into for example in the emails you wanted to create file document the dimensions of your first Matrix should be\n",
      "\n",
      "Text from chunk 3:\n",
      "what size into your topic size the second Matrix would be topics cross topics third Matrix would be topics cross documents how many of you understood this Matrix decomposition process ki contact ki documents you know everyone like all of you please you Unmute yourself exactly want you to tell me what is and what else if you see M is your number of documents and n is your number of words ok that will constitute your you know this term\n",
      "\n",
      "Text from chunk 4:\n",
      "term document Matrix right now you are breaking that Matrix into three mattresses and the algorithm will find out what should be the exact values in this mattresses OK what should be the values within your term words versus topics Matrix what should be the values within topics such as topic and what should be the numbers within your content was a document mattress for this term document Matrix you can pick anyone at the term Matrix short term frequency Matrix or TF idf Matrix which are one you like ok now let's look at a simple you know decomposition process this is your term frequency Matrix so a b c d e f these are the words these are the words these are the documents\n",
      "\n",
      "Text from chunk 5:\n",
      "ok is it making sense like how the term Matrix looks like OK then you are breaking that matrix A into three different matrices ok you are you Matrix represents your relation between your words and your topics here you are trying to create four topics F1 F2 F3 F4 ok and then the S represents the topic strength the S would be a diagonal Matrix ok the number represents for example at 23.1 that represents how strong the topic is in terms like you know if you given if you give a document if you classified into F1 topic pretty higher compared to the airport topic so it's a relative concept just the number how confident the algorithm means when creating that cluster\n",
      "\n",
      "Text from chunk 6:\n",
      "rising at given document into that particular topic ok and the last Matrix VT represents your occupant topic relation with each document this ideally says ok to which document does this topic for document belong to for example if you pick even what is the highest score within this column 33.37 right so this document belongs to F1 F1 topic and t2 what is the topic that it belongs to it belongs to F3 similarly belongs to F1 P4 belongs to which is the highest F1 again so ultimately the VT Matrix for your love you have a doubt ask me or else keep mute yourself\n",
      "\n",
      "Text from chunk 7:\n",
      "give me a second let me OK Google so this is how you are topics document decomposition looks like ok here you are you Matrix it shows the probability or you know the confidence are towards your topics and between your topics and words for example word number 1 it has what is the relation that it has with topic one and would be what is the relation that it has the topic one so if you look at this Matrix if you look at this column what she has the highest importance similarly\n",
      "\n",
      "Text from chunk 8:\n",
      "topic like you know it looks at the words you know DNB if d and d are present then the high chance that it belongs to F2 topic ok and also whenever you see minus it is trying to penalize ok if word is present in a dog topic then it shouldn't ideally belong to your F2 topics that the presence of word A will try to push the document\n",
      "\n",
      "Complete transcription saved to transcription.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Set the path for ffmpeg\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\ffmpeg\\ffmpeg-2024-10-24-git-153a6dc8fa-full_build\\bin'\n",
    "\n",
    "# Path to your MP3 file\n",
    "audio_path = r\"F:\\Testing for text\\audio_part_2.mp3\"\n",
    "\n",
    "# Convert MP3 to WAV (Mono, 16kHz)\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "audio = audio.set_channels(1)  # Convert to mono\n",
    "audio = audio.set_frame_rate(16000)  # Set the frame rate to 16kHz\n",
    "\n",
    "# Define chunk length in milliseconds (e.g., 1 minute per chunk = 60 seconds = 60000 milliseconds)\n",
    "chunk_length_ms = 60 * 1000  # 60 seconds (1 minute) per chunk\n",
    "total_chunks = math.ceil(len(audio) / chunk_length_ms)  # Total number of chunks\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to transcribe each chunk of audio\n",
    "def transcribe_chunk(audio_chunk, chunk_index):\n",
    "    chunk_path = f\"chunk{chunk_index}.wav\"\n",
    "    audio_chunk.export(chunk_path, format=\"wav\")  # Export each chunk as WAV\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            # Recognize the speech using Google's Speech Recognition API\n",
    "            text = recognizer.recognize_google(audio_data, language='en-US')\n",
    "            print(f\"Text from chunk {chunk_index}:\\n{text}\\n\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(f\"Google Speech Recognition could not understand chunk {chunk_index}\")\n",
    "            return \"\"\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service for chunk {chunk_index}; {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# Split audio into smaller chunks and transcribe each chunk\n",
    "all_text = \"\"\n",
    "for i in range(total_chunks):\n",
    "    start_time = i * chunk_length_ms\n",
    "    end_time = min((i + 1) * chunk_length_ms, len(audio))  # Make sure to not go beyond the audio length\n",
    "    audio_chunk = audio[start_time:end_time]  # Extract chunk\n",
    "    chunk_text = transcribe_chunk(audio_chunk, i)  # Transcribe the chunk\n",
    "    all_text += chunk_text + \" \"  # Append the text from each chunk\n",
    "\n",
    "# Save the complete transcription to a text file\n",
    "output_text_path = \"transcription.txt\"\n",
    "with open(output_text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(all_text)\n",
    "\n",
    "print(f\"Complete transcription saved to {output_text_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df873e22-f348-4b90-8c40-8fb27a06c9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796b80b-bae5-4920-a1f9-1085912345a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf4dbae-743c-4655-b280-e65812135ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_4.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_5.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_6.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_7.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_8.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_9.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in F:\\Testing for text\\audio_part_10.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Conversion completed. Audio files saved in F:\\Testing for text\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "# Input video path\n",
    "video_path = r\"D:\\chaitanya\\D drive\\OKEN SCANER\\DSA012\\AI-DSA012 Day-60 2nd Sept.mp4\"\n",
    "# Output folder for audio files\n",
    "output_folder = r\"F:\\Testing for text\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the video file\n",
    "video_clip = VideoFileClip(video_path)\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Calculate duration for each audio file\n",
    "total_duration = audio_clip.duration\n",
    "segment_duration = total_duration / 10  # Divide into 10 equal parts\n",
    "\n",
    "# Generate and save 10 audio files\n",
    "for i in range(10):\n",
    "    start_time = i * segment_duration\n",
    "    end_time = (i + 1) * segment_duration\n",
    "    audio_segment = audio_clip.subclip(start_time, end_time)\n",
    "    output_file_path = os.path.join(output_folder, f\"audio_part_{i + 1}.mp3\")\n",
    "    audio_segment.write_audiofile(output_file_path, codec=\"mp3\")\n",
    "\n",
    "# Close clips to release resources\n",
    "audio_clip.close()\n",
    "video_clip.close()\n",
    "\n",
    "print(\"Conversion completed. Audio files saved in\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaff5ad6-5a67-4dd1-b064-deadd56c678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from chunk 0:\n",
      "from your F2 topic is it making sense yes sir ok so if you see here the same logic goes for your science here as the like you know within the one document it is it has the highest probability to belong to F1 topic it has the lowest probability like you know like it does not belong if it is zero has no relation but if it is negative it says that it shouldn't belong it can belong to F4 this would be the probability that these are the chances that it can belong to this is how you decompose your Matrix so your a matrix your is your terms you can use your frequency Matrix or PF ID of matrix you need to break that into three different matrices the first Matrix represents\n",
      "\n",
      "Text from chunk 1:\n",
      "relation between the relationship between your words and topics the second Matrix represents how strong the topics are like you know with respect to each other and the third Matrix represents the topic relationship with your documents ok always remember that your as metrics would be a diagonal Matrix only that diagonal elements will be non zero rest of the elements would be zero ok is occurring six times these times is occurring two times f is occurring two times\n",
      "\n",
      "Text from chunk 2:\n",
      "ok so that's how like you know this is the numbers that represents the frequency of each word and this represents your relationship between your word and topic and remember the number of topics that it has to create is decided by you here I am deciding to create four topics tomorrow you can create three topics to topics whatever topics you want to create ok now the only Caveat is this LSA algorithm will not tell you what the topic is you have to decide what the topic is based upon your term and topic relationship for F1 topic\n",
      "\n",
      "Text from chunk 3:\n",
      "to understand ok it might be talking about a concept which is related to your CD and e similarly effort topic it gives importance to you to your and see so you need to understand OK it is giving importance to see and e so what does c and e mean and it is trying to penalize your B and F so you need to understand ok so what is this topic not talking about at all talking against OK so how you extract your meaning is completely up to you ok so this is how you are singular you know Matrix or value decomposition technique works STD works and this STD is the technique that is behind you know your electricity topic\n",
      "\n",
      "Text from chunk 4:\n",
      "stands for latent semantic analysis and your e stands for talk to the term document frequency it can be anything ok simple term Matrix or term frequency Matrix or TF idf Matrix always remember when you pick lekin your topics the way the topics are designed will completely change depending upon the matrix that cubic typically I would suggest you to speak Matrix because it by default already under weights are you right to reduce the importance of the words which occur in all the documents for even before applying LSA you are already doing some sort of topic modelling by reducing the weights which\n",
      "\n",
      "Text from chunk 5:\n",
      "already present in which are present in all the documents and giving importance to all the teachers to the words which are you know you need to set of documents so by default that TF idf itself will form a sort of clusters and your LSA will improvise the clusters and create a topic out of the if you use your term frequency still that would be fine but do not try to use your term Matrix which is just checking the presence of a word for example let's say the word marketing if it is occurring once how confident will I be to say that the email is talking about marketing should be lower compared to a scenario where if I count the number of times the word marketing is occurring at the marketing where is occurring 20 times yeah ok it is occurring 20 times then maybe it is talking about marketing topic if it is if I only check for whether the\n",
      "\n",
      "Text from chunk 6:\n",
      "occurring or not which would be binary that the conference will be quietly ok so try to pick either term Matrix or TF idf Matrix do not try to pick a matrix simple term Matrix ok so your age would be your term document Matrix and you Sigma and VT represents the three matrices that we just spoke of you represents your words to topics relationship between the confidence of each topic we transpose represents you know the relationship between your topics and documents ok so this is how you can send categories primary marketing categories\n",
      "\n",
      "Text from chunk 7:\n",
      "Vanakkam to clarity only males already customer marketing category promotion promotion category in the main primary calculation when you start you will not know the topics ok you cannot decide what the topic represents you will start creating two topics for three topics of\n",
      "\n",
      "Text from chunk 8:\n",
      "break the entire document set into different topics for topics ok now you got your usn within us and VT mattresses with you ok VT mattresses is talking about ok which topic does it belong to ok and you are you Matrix talks about ok within each topic which words are given which words are given importance\n",
      "\n",
      "Complete transcription saved to transcription.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Set the path for ffmpeg\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\ffmpeg\\ffmpeg-2024-10-24-git-153a6dc8fa-full_build\\bin'\n",
    "\n",
    "# Path to your MP3 file\n",
    "audio_path = r\"F:\\Testing for text\\audio_part_3.mp3\"\n",
    "\n",
    "# Convert MP3 to WAV (Mono, 16kHz)\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "audio = audio.set_channels(1)  # Convert to mono\n",
    "audio = audio.set_frame_rate(16000)  # Set the frame rate to 16kHz\n",
    "\n",
    "# Define chunk length in milliseconds (e.g., 1 minute per chunk = 60 seconds = 60000 milliseconds)\n",
    "chunk_length_ms = 60 * 1000  # 60 seconds (1 minute) per chunk\n",
    "total_chunks = math.ceil(len(audio) / chunk_length_ms)  # Total number of chunks\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to transcribe each chunk of audio\n",
    "def transcribe_chunk(audio_chunk, chunk_index):\n",
    "    chunk_path = f\"chunk{chunk_index}.wav\"\n",
    "    audio_chunk.export(chunk_path, format=\"wav\")  # Export each chunk as WAV\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            # Recognize the speech using Google's Speech Recognition API\n",
    "            text = recognizer.recognize_google(audio_data, language='en-US')\n",
    "            print(f\"Text from chunk {chunk_index}:\\n{text}\\n\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(f\"Google Speech Recognition could not understand chunk {chunk_index}\")\n",
    "            return \"\"\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service for chunk {chunk_index}; {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# Split audio into smaller chunks and transcribe each chunk\n",
    "all_text = \"\"\n",
    "for i in range(total_chunks):\n",
    "    start_time = i * chunk_length_ms\n",
    "    end_time = min((i + 1) * chunk_length_ms, len(audio))  # Make sure to not go beyond the audio length\n",
    "    audio_chunk = audio[start_time:end_time]  # Extract chunk\n",
    "    chunk_text = transcribe_chunk(audio_chunk, i)  # Transcribe the chunk\n",
    "    all_text += chunk_text + \" \"  # Append the text from each chunk\n",
    "\n",
    "# Save the complete transcription to a text file\n",
    "output_text_path = \"transcription.txt\"\n",
    "with open(output_text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(all_text)\n",
    "\n",
    "print(f\"Complete transcription saved to {output_text_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0879e7-e40d-4ce0-9827-f6c2bf640bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
