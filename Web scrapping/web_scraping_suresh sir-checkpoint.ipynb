{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBgqYrlRTtnz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# URL of the website to scrape\n",
        "url = \"https://www.imdb.com/\"\n",
        "\n",
        "# Send an HTTP GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "else:\n",
        "    # Parse the HTML code using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the relevant information from the HTML code\n",
        "    movies = []\n",
        "    rows = soup.select('tbody.lister-list tr')\n",
        "\n",
        "    # Check if rows are found\n",
        "    if not rows:\n",
        "        print(\"No rows found in the table. Please check the selectors.\")\n",
        "    else:\n",
        "        for row in rows:\n",
        "            title = row.find('td', class_='titleColumn').find('a').get_text()\n",
        "            year = row.find('td', class_='titleColumn').find('span', class_='secondaryInfo').get_text()[1:-1]\n",
        "            rating = row.find('td', class_='ratingColumn imdbRating').find('strong').get_text()\n",
        "            movies.append([title, year, rating])\n",
        "\n",
        "        # Store the information in a pandas dataframe\n",
        "        df = pd.DataFrame(movies, columns=['Title', 'Year', 'Rating'])\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            print(\"The DataFrame is empty. Please check the data extraction logic.\")\n",
        "        else:\n",
        "            print(df)\n",
        "\n",
        "    # Add a delay between requests to avoid overwhelming the website with requests\n",
        "    time.sleep(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 lxml -q"
      ],
      "metadata": {
        "id": "YzKxfWEuT9gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        " # The numbers are lat-lon of New York\n",
        "URL = \"https://weather.com/weather/today/l/40.75,-73.98\"\n",
        "resp = requests.get(URL)\n",
        "print(resp.status_code)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "id": "TUVvmiHoVLKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import io\n",
        " import pandas as pd\n",
        " import requests\n",
        " URL = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=T10YIE&cosd=2017-04-14&coedâŒ‹ =2022-04-14\"\n",
        " resp = requests.get(URL)\n",
        " if resp.status_code == 200:\n",
        "    print('Data is readble')\n",
        "    csvtext = resp.text\n",
        "    csvbuffer = io.StringIO(csvtext)\n",
        "    df = pd.read_csv(csvbuffer)\n",
        "    print(df)"
      ],
      "metadata": {
        "id": "SpANA1iCVWKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "URL = \"https://books.toscrape.com/\"\n",
        "resp = requests.get(URL)\n",
        "print(resp.status_code)\n"
      ],
      "metadata": {
        "id": "FjeDQSMKVnge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import requests\n",
        " from lxml import etree\n",
        " from bs4 import BeautifulSoup\n",
        " # Reading temperature of New York\n",
        " URL = \"https://weather.com/weather/today/l/40.75,-73.98\"\n",
        " resp = requests.get(URL)\n",
        " if resp.status_code == 200:\n",
        "    # Using lxml\n",
        "    dom = etree.HTML(resp.text)\n",
        "    elements = dom.xpath(\"//span[@data-testid='TemperatureValue' and \" \\\n",
        "    \"contains(@class,'CurrentConditions')]\")\n",
        "    print(elements[0].text)\n",
        " # Using BeautifulSoup\n",
        " soup = BeautifulSoup(resp.text, \"lxml\")\n",
        " elements = soup.select('span[data-testid=\"TemperatureValue\"]' \\\n",
        " '[class^=\"CurrentConditions\"]')\n",
        " print(elements[0].text)\n"
      ],
      "metadata": {
        "id": "fY0YY6f3V0nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data from HTML"
      ],
      "metadata": {
        "id": "_MRi2rhK9i7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tables = pd.read_html(\"https://www.federalreserve.gov/releases/h15/\")\n",
        "print(tables)"
      ],
      "metadata": {
        "id": "ABooBcXPWpvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from lxml import etree\n",
        " import requests\n",
        " # Read Yahoo home page\n",
        " URL = \"https://www.yahoo.com/\"\n",
        " resp = requests.get(URL)\n",
        " dom = etree.HTML(resp.text)\n",
        " # Print news headlines\n",
        " elements = dom.xpath(\"//h3/a[u[@class='StretchedBox']]\")\n",
        " for elem in elements:\n",
        "      print(etree.tostring(elem, method=\"text\", encoding=\"unicode\"))"
      ],
      "metadata": {
        "id": "wCX9196dW3Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "id": "2YNsZ16yXX6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n"
      ],
      "metadata": {
        "id": "m3R_u6GFhg7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install chromium-driver"
      ],
      "metadata": {
        "id": "ghDp_NxGhmJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_webdriver():\n",
        "    # Setup Chrome options for headless mode\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--window-size=1920,1080\")\n",
        "    options.add_argument(\"--disable-extensions\")\n",
        "    options.add_argument(\"--disable-software-rasterizer\")\n",
        "\n",
        "    # Return a new WebDriver instance with the specified Chrome options\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver\n"
      ],
      "metadata": {
        "id": "cvNfJqp8h7b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver = web_webdriver()"
      ],
      "metadata": {
        "id": "RnRaR6MiiXMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zdImmFXF5zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By"
      ],
      "metadata": {
        "id": "5yacfI1JjMrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver.get('https://www.flipkart.com/')"
      ],
      "metadata": {
        "id": "660OqtgVigv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "containers= driver.find_elements(By.XPATH,'//article[@class=\"product_pod\"]')"
      ],
      "metadata": {
        "id": "gjphEdQti2Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for container in containers:\n",
        "    image = container.find_element(By.XPATH, './/div[@class=\"image_container\"]/a/img')\n",
        "    image_url = image.get_attribute('src')  # Assuming you want to retrieve the 'src' attribute instead of 'href'\n",
        "    link= container.find_element(By.XPATH, './/div[@class=\"image_container\"]/a')\n",
        "    link_url = link.get_attribute('href')\n",
        "    title =container.find_element(By.XPATH, './/h3/a').text\n",
        "    price = container.find_element(By.XPATH, './/p[@class=\"price_color\"]').text\n",
        "    stock= container.find_element(By.XPATH, './/p[@class=\"instock availability\"]').text\n",
        "    sh.append_row([image_url,link_url,title,price,stock])\n",
        "    print(image_url,link_url,title,price,stock)\n"
      ],
      "metadata": {
        "id": "OaZnT7_RljyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gspread"
      ],
      "metadata": {
        "id": "NDk8BVA4l1Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread"
      ],
      "metadata": {
        "id": "JmZUwrpxpGFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.auth import default"
      ],
      "metadata": {
        "id": "TRB5mI4rpT8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creds,_ = default()"
      ],
      "metadata": {
        "id": "PUL55Oz9pfXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "tOX1Znv2ppng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs = gc.create('scrap the books_D06062024')"
      ],
      "metadata": {
        "id": "riIIIOUJqC7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sh= gs.sheet1"
      ],
      "metadata": {
        "id": "Qpfn_SlEqOlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "guuCeVC9qfwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}