{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zBgqYrlRTtnz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.imdb.com/\"\n",
    "\n",
    "# Send an HTTP GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "else:\n",
    "    # Parse the HTML code using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the relevant information from the HTML code\n",
    "    movies = []\n",
    "    rows = soup.select('tbody.lister-list tr')\n",
    "\n",
    "    # Check if rows are found\n",
    "    if not rows:\n",
    "        print(\"No rows found in the table. Please check the selectors.\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            title = row.find('td', class_='titleColumn').find('a').get_text()\n",
    "            year = row.find('td', class_='titleColumn').find('span', class_='secondaryInfo').get_text()[1:-1]\n",
    "            rating = row.find('td', class_='ratingColumn imdbRating').find('strong').get_text()\n",
    "            movies.append([title, year, rating])\n",
    "\n",
    "        # Store the information in a pandas dataframe\n",
    "        df = pd.DataFrame(movies, columns=['Title', 'Year', 'Rating'])\n",
    "\n",
    "        # Check if DataFrame is empty\n",
    "        if df.empty:\n",
    "            print(\"The DataFrame is empty. Please check the data extraction logic.\")\n",
    "        else:\n",
    "            print(df)\n",
    "\n",
    "    # Add a delay between requests to avoid overwhelming the website with requests\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YzKxfWEuT9gw"
   },
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 lxml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TUVvmiHoVLKa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    " # The numbers are lat-lon of New York\n",
    "URL = \"https://weather.com/weather/today/l/40.75,-73.98\"\n",
    "resp = requests.get(URL)\n",
    "print(resp.status_code)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SpANA1iCVWKW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is readble\n",
      "            DATE T10YIE\n",
      "0     2017-04-17   1.88\n",
      "1     2017-04-18   1.85\n",
      "2     2017-04-19   1.85\n",
      "3     2017-04-20   1.85\n",
      "4     2017-04-21   1.84\n",
      "...          ...    ...\n",
      "1925  2024-09-02      .\n",
      "1926  2024-09-03   2.11\n",
      "1927  2024-09-04   2.07\n",
      "1928  2024-09-05   2.03\n",
      "1929  2024-09-06   2.03\n",
      "\n",
      "[1930 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    " import io\n",
    " import pandas as pd\n",
    " import requests\n",
    " URL = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=T10YIE&cosd=2017-04-14&coed⌋ =2022-04-14\"\n",
    " resp = requests.get(URL)\n",
    " if resp.status_code == 200:\n",
    "    print('Data is readble')\n",
    "    csvtext = resp.text\n",
    "    csvbuffer = io.StringIO(csvtext)\n",
    "    df = pd.read_csv(csvbuffer)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FjeDQSMKVnge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "URL = \"https://books.toscrape.com/\"\n",
    "resp = requests.get(URL)\n",
    "print(resp.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fY0YY6f3V0nn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "59°\n"
     ]
    }
   ],
   "source": [
    " import requests\n",
    " from lxml import etree\n",
    " from bs4 import BeautifulSoup\n",
    " # Reading temperature of New York\n",
    " URL = \"https://weather.com/weather/today/l/40.75,-73.98\"\n",
    " resp = requests.get(URL)\n",
    " if resp.status_code == 200:\n",
    "    # Using lxml\n",
    "    dom = etree.HTML(resp.text)\n",
    "    elements = dom.xpath(\"//span[@data-testid='TemperatureValue' and \" \\\n",
    "    \"contains(@class,'CurrentConditions')]\")\n",
    "    print(elements[0].text)\n",
    " # Using BeautifulSoup\n",
    " soup = BeautifulSoup(resp.text, \"lxml\")\n",
    " elements = soup.select('span[data-testid=\"TemperatureValue\"]' \\\n",
    " '[class^=\"CurrentConditions\"]')\n",
    " print(elements[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MRi2rhK9i7-"
   },
   "source": [
    "Read the data from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ABooBcXPWpvM"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tables \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.federalreserve.gov/releases/h15/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tables)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1225\u001b[0m     [\n\u001b[0;32m   1226\u001b[0m         is_file_like(io),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     ]\n\u001b[0;32m   1231\u001b[0m ):\n\u001b[0;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1238\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1241\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[0;32m   1242\u001b[0m     io\u001b[38;5;241m=\u001b[39mio,\n\u001b[0;32m   1243\u001b[0m     match\u001b[38;5;241m=\u001b[39mmatch,\n\u001b[0;32m   1244\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   1245\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   1246\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m   1247\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m   1248\u001b[0m     thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m   1249\u001b[0m     attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1251\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1252\u001b[0m     converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m   1253\u001b[0m     na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m   1254\u001b[0m     keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n\u001b[0;32m   1255\u001b[0m     displayed_only\u001b[38;5;241m=\u001b[39mdisplayed_only,\n\u001b[0;32m   1256\u001b[0m     extract_links\u001b[38;5;241m=\u001b[39mextract_links,\n\u001b[0;32m   1257\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1258\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1259\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:983\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    972\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[0;32m    973\u001b[0m     io,\n\u001b[0;32m    974\u001b[0m     compiled_match,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    979\u001b[0m     storage_options,\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 983\u001b[0m     tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse_tables()\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_doc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:806\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    804\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(r, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_content\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:785\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio):\n\u001b[1;32m--> 785\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    786\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options\n\u001b[0;32m    787\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    788\u001b[0m             r \u001b[38;5;241m=\u001b[39m parse(f\u001b[38;5;241m.\u001b[39mhandle, parser\u001b[38;5;241m=\u001b[39mparser)\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# try to parse the input in the simplest way\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    729\u001b[0m     path_or_buf,\n\u001b[0;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tables = pd.read_html(\"https://www.federalreserve.gov/releases/h15/\")\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wCX9196dW3Ob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Amazon deals this weekend include an Apple iPad for its all-time lowest price\n",
      "When does Apple launch the new iPhone 16? Here's everything you need to know\n",
      "The iOS 18 release date is quickly approaching but is your iPhone compatible? Here are the eligible devices and new features\n",
      "The best bras of 2024: 16 tested, pro-approved styles for comfort and support\n",
      "Lenovo announces new Aura edition laptops\n",
      "Best single-serve coffee makers of 2024, tested and reviewed\n",
      "Apple iPhone 16 event: What to expect from the AI-packed 'Glowtime' show\n",
      "On wild day for Dak Prescott, Browns show Cowboys and their QB why megadeal is a win for both\n",
      "Time to bench Deshaun, Patriots shock the world: Week 1 instant reactions | Inside Coverage\n",
      "Simone Biles 'almost had a heart attack' after husband Jonathan Owens scored first Bears TD of the season\n",
      "Bo Nix throws 2 interceptions in Broncos debut, prompts audible 'oh no' from CBS announcer in loss to Seahawks\n",
      "Joey Bosa 'shocked' Raiders punted on 4th-and-1 inside Chargers territory\n",
      "Bengals star Ja'Marr Chase frustrated with usage after loss to Patriots, also says he had food poisoning\n",
      "Patriots stun Bengals with 16-10 upset win to knock out more than 43% of survivor pool entries\n",
      "NFL Winners and Losers: Bryce Young falls flat and Panthers get embarrassed by Saints\n",
      "Chargers’ Joshua Palmer, Raiders’ Jack Jones ejected after fight breaks out late in Jim Harbaugh’s debut\n",
      "Kirk Cousins’ rough start eliminates the Falcons’ benefit of the doubt for 2024\n",
      "Week 1 Fantasy Football Booms & Busts: C.J. Stroud, Texans don't get the sloppy play memo\n",
      "Week 1 Care/Don't Care: Jameson Williams finally erupts\n",
      "Fantasy Football Waiver Wire: Early pickups for Week 2\n",
      "Fantasy Football Week 1 Wrap: Jayden Daniels is officially top-10\n",
      "Lions' season of hope is full speed ahead after opening with an electric overtime win vs. Rams\n",
      "Alex Morgan bids emotional farewell in final game of her soccer career\n",
      "Shohei Ohtani hits 46th home run to match his 46 stolen bases, charging toward 50-50 season\n",
      "Lions flirt with upset loss to Rams in their opener, but run game leads a win in OT\n",
      "NASCAR: Joey Logano wins playoff opener at Atlanta\n",
      "Consumer Reports readers say these brands build the best used cars\n",
      "Patriots defense comes up with wild goal-line play to prevent Bengals TD in first-half shutout of Cincinnati\n",
      "Jayden Daniels doesn't make much happen in a lackluster NFL debut\n",
      "Fantasy Football Week 1 Pulse Check: Kirk Cousins, Falcons lead biggest disappointments\n",
      "Biggest takeaways from Nebraska's embarrassing beatdown of Colorado\n",
      "Rams WR Puka Nacua leaves OT loss to Lions early with knee injury\n",
      "Tony's takes: My thoughts on Alabama's uninspiring win over South Florida\n",
      "Dak Prescott, Cowboys agree to record-breaking four-year, $240M contract extension\n",
      "Russell Wilson ruled out for Steelers debut, Justin Fields to start instead\n",
      "Mortgage and refinance rates today, September 8, 2024: Rates under 6% for 2 consecutive weeks\n",
      "Anthony Richardson's breathtaking 60-yard touchdown not enough for the Colts win\n",
      "'I Have Not Emotionally Or Legally Adopted You': Warren Buffett Cut Off His Granddaughter And She Couldn't Afford Cable Or Health Insurance\n",
      "Patti Scialfa, Bruce Springsteen’s Wife and E Street Bandmate, Reveals Blood Cancer Diagnosis\n",
      "Shedeur Sanders' immaturity on full display at end of Colorado's blowout loss\n",
      "Alleged Georgia School Shooter Colt Gray’s Dad Is ‘Evil,’ Grandpa Says\n",
      "Taylor Swift, Travis Kelce Attend U.S. Open — 22 Years After 12-Year-Old Swift Sang ‘America the Beautiful’ There\n",
      "'Very Troubled And Very Desperate': Trump Biographer Spots 1 Key Change\n",
      "Fans Express Concern for Dave Bautista After He Looks Unrecognizable in New Photos\n",
      "Taylor Swift Makes the Case For Breaking Traditional Wedding Dress Codes With Travis Kelce\n",
      "Donald Trump Botches New BFF Elon Musk’s Name And You Know What Happened\n",
      "Melania Trump Breaks Silence to Complain About 2020 Defeat\n",
      "California man loses life savings, owes more than $30K in taxes after falling prey to sophisticated scam\n",
      "Trump calls Elon Musk by wrong name as his biographer says the candidate is 'hyper-aware' voters are doubting his mental acuity\n",
      "Kimberly Guilfoyle forced to ask audience to clap as awkward speech falls flat\n",
      "Elton John's Kids ‘Worry About My Mortality’ at 77: 'I Don't Think I'm Going to Be Around for' Big Life Moments\n",
      "Trump accuses Harris of hiding from media and revives election fraud narrative ahead of crucial debate: Live\n",
      "'Oh My God': James Carville Unleashes On Nikki Haley For Backing Trump\n",
      "Ben Affleck’s Rarely-Seen Son Samuel’s Latest Ensemble Has the Sweetest Nod to Big Sis Violet\n",
      "‘World’s deadliest chick’ hatches in Cotswolds bird park\n",
      "Malia Obama Makes Rare Red Carpet Appearance at French Film Festival: ‘A Little Bit Terrified’\n",
      "Matt Lauer's Daughter Romy Crashed Car into Hamptons Fence, Left Behind Part of License Plate\n",
      "Caleb Williams stats: How did Bears star rookie do in NFL debut?\n",
      "Studs and duds from Bears' comeback win vs. Titans\n",
      "Simone Biles Had the Best Reaction to Husband Jonathan Owens Scoring a Touchdown in His First Game with the Bears\n",
      "Cooper says he’s ‘tired of seeing texts’ from students telling parents they’re afraid of school shootings. ‘We cannot normalize this.’\n",
      "32 things we learned in NFL Week 1: Top players, teams make opening statements\n",
      "Here's What 27 Famous Long-Term Celebrity Couples Looked Like When They First Started Dating Vs. Now\n",
      "Viral Instagram posts and TikToks show how toxic photo editing apps can be\n",
      "College football rankings: SEC teams in updated Coaches Poll, AP Top 25 after Week 2\n",
      "AP Top 25: SEC grabs six of the first seven spots in rankings as Notre Dame tumbles to No. 18\n",
      "MLB trade deadline revisited: Dodgers pulled off heist to get new bullpen ace\n",
      "College football rankings: Big Ten teams in updated Coaches Poll, AP Top 25 after Week 2\n",
      "Heather Thomas: How a TV star evolved into a behind-the-scenes politico\n",
      "E Street Band Member Patti Scialfa Reveals 2018 Multiple Myeloma Diagnosis: 'Touring Has Become a Challenge'\n",
      "The Bush-Cheney dynasty isn't backing Trump. There's a time that would have been unthinkable\n",
      "Warren Buffett's $5.4 Billion Warning to Wall Street Foreshadows Trouble to Come\n",
      "NFL Week 1 winners, losers: Ex-49er Darnold stars in Vikings' win\n",
      "What we learned as Bears' defense bails out Caleb Williams, offense in comeback win vs. Titans\n",
      "Mike Rogers says Republicans will make 2024 election ‘too big to rig’\n",
      "Lakers invited veteran wing to training camp, but he said no\n",
      "This controversial Bradley Cooper movie is a big hit on Netflix. Is it worth streaming?\n",
      "Emily Blunt and John Krasinski Make Rare Appearance with Daughters Hazel, 10, and Violet, 7, at US Open Women's Final\n",
      "Story behind 49ers not trading Brandon Aiyuk to Steelers is insane\n"
     ]
    }
   ],
   "source": [
    " from lxml import etree\n",
    " import requests\n",
    " # Read Yahoo home page\n",
    " URL = \"https://www.yahoo.com/\"\n",
    " resp = requests.get(URL)\n",
    " dom = etree.HTML(resp.text)\n",
    " # Print news headlines\n",
    " elements = dom.xpath(\"//h3/a[u[@class='StretchedBox']]\")\n",
    " for elem in elements:\n",
    "      print(etree.tostring(elem, method=\"text\", encoding=\"unicode\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2YNsZ16yXX6s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\chaitanya\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "m3R_u6GFhg7K"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ghDp_NxGhmJ8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install chromium-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cvNfJqp8h7b7"
   },
   "outputs": [],
   "source": [
    "def web_webdriver():\n",
    "    # Setup Chrome options for headless mode\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--disable-software-rasterizer\")\n",
    "\n",
    "    # Return a new WebDriver instance with the specified Chrome options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnRaR6MiiXMQ"
   },
   "outputs": [],
   "source": [
    "driver = web_webdriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zdImmFXF5zs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yacfI1JjMrG"
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "660OqtgVigv3"
   },
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjphEdQti2Ua"
   },
   "outputs": [],
   "source": [
    "containers= driver.find_elements(By.XPATH,'//article[@class=\"product_pod\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaZnT7_RljyB"
   },
   "outputs": [],
   "source": [
    "for container in containers:\n",
    "    image = container.find_element(By.XPATH, './/div[@class=\"image_container\"]/a/img')\n",
    "    image_url = image.get_attribute('src')  # Assuming you want to retrieve the 'src' attribute instead of 'href'\n",
    "    link= container.find_element(By.XPATH, './/div[@class=\"image_container\"]/a')\n",
    "    link_url = link.get_attribute('href')\n",
    "    title =container.find_element(By.XPATH, './/h3/a').text\n",
    "    price = container.find_element(By.XPATH, './/p[@class=\"price_color\"]').text\n",
    "    stock= container.find_element(By.XPATH, './/p[@class=\"instock availability\"]').text\n",
    "    sh.append_row([image_url,link_url,title,price,stock])\n",
    "    print(image_url,link_url,title,price,stock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDk8BVA4l1Ow"
   },
   "outputs": [],
   "source": [
    "!pip install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmZUwrpxpGFS"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRB5mI4rpT8X"
   },
   "outputs": [],
   "source": [
    "from google.auth import default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUL55Oz9pfXZ"
   },
   "outputs": [],
   "source": [
    "creds,_ = default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOX1Znv2ppng"
   },
   "outputs": [],
   "source": [
    "gc = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riIIIOUJqC7s"
   },
   "outputs": [],
   "source": [
    "gs = gc.create('scrap the books_D06062024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpfn_SlEqOlQ"
   },
   "outputs": [],
   "source": [
    "sh= gs.sheet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guuCeVC9qfwg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
